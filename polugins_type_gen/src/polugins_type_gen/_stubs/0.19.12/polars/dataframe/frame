from typing_extensions import ParamSpec, Generic
import deltalake

from datetime import timedelta
from io import BytesIO, IOBase, TextIOWrapper
from pathlib import Path
from polars import Expr as Expr, LazyFrame as LazyFrame, Series as Series
from polars.dataframe._html import NotebookFormatter as NotebookFormatter
from polars.dataframe.group_by import DynamicGroupBy as DynamicGroupBy, GroupBy as GroupBy, RollingGroupBy as RollingGroupBy
from polars.datatypes import Boolean as Boolean, FLOAT_DTYPES as FLOAT_DTYPES, Float64 as Float64, INTEGER_DTYPES as INTEGER_DTYPES, NUMERIC_DTYPES as NUMERIC_DTYPES, N_INFER_DEFAULT as N_INFER_DEFAULT, Object as Object, Utf8 as Utf8, py_type_to_dtype as py_type_to_dtype
from polars.dependencies import _PYARROW_AVAILABLE as _PYARROW_AVAILABLE, _check_for_numpy as _check_for_numpy, _check_for_pandas as _check_for_pandas, _check_for_pyarrow as _check_for_pyarrow, dataframe_api_compat as dataframe_api_compat, numpy as np, pandas as pd, pyarrow as pa
from polars.exceptions import NoRowsReturnedError as NoRowsReturnedError, TooManyRowsReturnedError as TooManyRowsReturnedError
from polars.functions import col as col, lit as lit
from polars.interchange.dataframe import PolarsDataFrame as PolarsDataFrame
from polars.io._utils import _is_glob_pattern as _is_glob_pattern, _is_local_file as _is_local_file
from polars.io.csv._utils import _check_arg_is_1byte as _check_arg_is_1byte
from polars.io.spreadsheet._write_utils import _XLFormatCache as _XLFormatCache, _unpack_multi_column_dict as _unpack_multi_column_dict, _xl_apply_conditional_formats as _xl_apply_conditional_formats, _xl_inject_sparklines as _xl_inject_sparklines, _xl_setup_table_columns as _xl_setup_table_columns, _xl_setup_table_options as _xl_setup_table_options, _xl_setup_workbook as _xl_setup_workbook, _xl_unique_table_name as _xl_unique_table_name
from polars.polars import PyDataFrame as PyDataFrame
from polars.selectors import _expand_selector_dicts as _expand_selector_dicts, _expand_selectors as _expand_selectors
from polars.slice import PolarsSlice as PolarsSlice
from polars.type_aliases import AsofJoinStrategy as AsofJoinStrategy, AvroCompression as AvroCompression, ClosedInterval as ClosedInterval, ColumnFormatDict as ColumnFormatDict, ColumnNameOrSelector as ColumnNameOrSelector, ColumnTotalsDefinition as ColumnTotalsDefinition, ColumnWidthsDefinition as ColumnWidthsDefinition, ComparisonOperator as ComparisonOperator, ConditionalFormatDict as ConditionalFormatDict, CsvEncoding as CsvEncoding, CsvQuoteStyle as CsvQuoteStyle, DbWriteEngine as DbWriteEngine, DbWriteMode as DbWriteMode, FillNullStrategy as FillNullStrategy, FrameInitTypes as FrameInitTypes, IndexOrder as IndexOrder, IntoExpr as IntoExpr, IntoExprColumn as IntoExprColumn, IpcCompression as IpcCompression, JoinStrategy as JoinStrategy, JoinValidation as JoinValidation, Label as Label, NullStrategy as NullStrategy, OneOrMoreDataTypes as OneOrMoreDataTypes, Orientation as Orientation, ParallelStrategy as ParallelStrategy, ParquetCompression as ParquetCompression, PivotAgg as PivotAgg, PolarsDataType as PolarsDataType, RollingInterpolationMethod as RollingInterpolationMethod, RowTotalsDefinition as RowTotalsDefinition, SchemaDefinition as SchemaDefinition, SchemaDict as SchemaDict, SelectorType as SelectorType, SizeUnit as SizeUnit, StartBy as StartBy, UniqueKeepStrategy as UniqueKeepStrategy, UnstackDirection as UnstackDirection
from polars.utils._construction import _post_apply_columns as _post_apply_columns, arrow_to_pydf as arrow_to_pydf, dict_to_pydf as dict_to_pydf, iterable_to_pydf as iterable_to_pydf, numpy_to_idxs as numpy_to_idxs, numpy_to_pydf as numpy_to_pydf, pandas_to_pydf as pandas_to_pydf, sequence_to_pydf as sequence_to_pydf, series_to_pydf as series_to_pydf
from polars.utils._parse_expr_input import parse_as_expression as parse_as_expression
from polars.utils._wrap import wrap_expr as wrap_expr, wrap_ldf as wrap_ldf, wrap_s as wrap_s
from polars.utils.convert import _timedelta_to_pl_duration as _timedelta_to_pl_duration
from polars.utils.deprecation import deprecate_function as deprecate_function, deprecate_nonkeyword_arguments as deprecate_nonkeyword_arguments, deprecate_renamed_function as deprecate_renamed_function, deprecate_renamed_parameter as deprecate_renamed_parameter
from polars.utils.various import _prepare_row_count_args as _prepare_row_count_args, _process_null_values as _process_null_values, can_create_dicts_with_pyarrow as can_create_dicts_with_pyarrow, handle_projection_columns as handle_projection_columns, is_bool_sequence as is_bool_sequence, is_int_sequence as is_int_sequence, is_str_sequence as is_str_sequence, normalize_filepath as normalize_filepath, parse_percentiles as parse_percentiles, parse_version as parse_version, range_to_slice as range_to_slice, scale_bytes as scale_bytes
from typing import Any, BinaryIO, Callable, ClassVar, Collection, Concatenate, Iterable, Iterator, Literal, Mapping, NoReturn, Sequence, TypeAlias, TypeVar, overload
from typing_extensions import Self
from xlsxwriter import Workbook as Workbook

MultiRowSelector: TypeAlias
MultiColSelector: TypeAlias
T = TypeVar('T')
P = ParamSpec('P')

class DataFrame(Generic[P]):
    _accessors: ClassVar[set[str]]
    _df: PyDataFrame
    def __init__(self, data: FrameInitTypes | None = ..., schema: SchemaDefinition | None = ..., *, schema_overrides: SchemaDict | None = ..., orient: Orientation | None = ..., infer_schema_length: int | None = ..., nan_to_null: bool = ...) -> None: ...
    @classmethod
    def _from_pydf(cls, py_df: PyDataFrame) -> Self: ...
    @classmethod
    def _from_dicts(cls, data: Sequence[dict[str, Any]], schema: SchemaDefinition | None = ..., *, schema_overrides: SchemaDict | None = ..., infer_schema_length: int | None = ...) -> Self: ...
    @classmethod
    def _from_dict(cls, data: Mapping[str, Sequence[object] | Mapping[str, Sequence[object]] | Series], schema: SchemaDefinition | None = ..., *, schema_overrides: SchemaDict | None = ...) -> Self: ...
    @classmethod
    def _from_records(cls, data: Sequence[Any], schema: SchemaDefinition | None = ..., *, schema_overrides: SchemaDict | None = ..., orient: Orientation | None = ..., infer_schema_length: int | None = ...) -> Self: ...
    @classmethod
    def _from_numpy(cls, data: np.ndarray[Any, Any], schema: SchemaDefinition | None = ..., *, schema_overrides: SchemaDict | None = ..., orient: Orientation | None = ...) -> Self: ...
    @classmethod
    def _from_arrow(cls, data: pa.Table, schema: SchemaDefinition | None = ..., *, schema_overrides: SchemaDict | None = ..., rechunk: bool = ...) -> Self: ...
    @classmethod
    def _from_pandas(cls, data: pd.DataFrame, schema: SchemaDefinition | None = ..., *, schema_overrides: SchemaDict | None = ..., rechunk: bool = ..., nan_to_null: bool = ..., include_index: bool = ...) -> Self: ...
    @classmethod
    def _read_csv(cls, source: str | Path | BinaryIO | bytes, *, has_header: bool = ..., columns: Sequence[int] | Sequence[str] | None = ..., separator: str = ..., comment_char: str | None = ..., quote_char: str | None = ..., skip_rows: int = ..., dtypes: None | SchemaDict | Sequence[PolarsDataType] = ..., schema: None | SchemaDict = ..., null_values: str | Sequence[str] | dict[str, str] | None = ..., missing_utf8_is_empty_string: bool = ..., ignore_errors: bool = ..., try_parse_dates: bool = ..., n_threads: int | None = ..., infer_schema_length: int | None = ..., batch_size: int = ..., n_rows: int | None = ..., encoding: CsvEncoding = ..., low_memory: bool = ..., rechunk: bool = ..., skip_rows_after_header: int = ..., row_count_name: str | None = ..., row_count_offset: int = ..., sample_size: int = ..., eol_char: str = ..., raise_if_empty: bool = ..., truncate_ragged_lines: bool = ...) -> DataFrame: ...
    @classmethod
    def _read_parquet(cls, source: str | Path | BinaryIO | bytes, *, columns: Sequence[int] | Sequence[str] | None = ..., n_rows: int | None = ..., parallel: ParallelStrategy = ..., row_count_name: str | None = ..., row_count_offset: int = ..., low_memory: bool = ..., use_statistics: bool = ..., rechunk: bool = ...) -> DataFrame: ...
    @classmethod
    def _read_avro(cls, source: str | Path | BinaryIO | bytes, *, columns: Sequence[int] | Sequence[str] | None = ..., n_rows: int | None = ...) -> Self: ...
    @classmethod
    def _read_ipc(cls, source: str | Path | BinaryIO | bytes, *, columns: Sequence[int] | Sequence[str] | None = ..., n_rows: int | None = ..., row_count_name: str | None = ..., row_count_offset: int = ..., rechunk: bool = ..., memory_map: bool = ...) -> Self: ...
    @classmethod
    def _read_ipc_stream(cls, source: str | Path | BinaryIO | bytes, *, columns: Sequence[int] | Sequence[str] | None = ..., n_rows: int | None = ..., row_count_name: str | None = ..., row_count_offset: int = ..., rechunk: bool = ...) -> Self: ...
    @classmethod
    def _read_json(cls, source: str | Path | IOBase | bytes, *, infer_schema_length: int | None = ..., schema: SchemaDefinition | None = ..., schema_overrides: SchemaDefinition | None = ...) -> Self: ...
    @classmethod
    def _read_ndjson(cls, source: str | Path | IOBase | bytes, *, schema: SchemaDefinition | None = ..., schema_overrides: SchemaDefinition | None = ..., ignore_errors: bool = ...) -> Self: ...
    def _replace(self, column: str, new_column: Series) -> Self: ...
    @property
    def shape(self) -> tuple[int, int]: ...
    @property
    def height(self) -> int: ...
    @property
    def width(self) -> int: ...
    @property
    def columns(self) -> list[str]: ...
    @property
    def dtypes(self) -> list[PolarsDataType]: ...
    @property
    def flags(self) -> dict[str, dict[str, bool]]: ...
    @property
    def schema(self) -> SchemaDict: ...
    def __array__(self, dtype: Any = ...) -> np.ndarray[Any, Any]: ...
    def __dataframe__(self, nan_as_null: bool = ..., allow_copy: bool = ...) -> PolarsDataFrame: ...
    def __dataframe_consortium_standard__(self, *, api_version: str | None = ...) -> Any: ...
    def _comp(self, other: Any, op: ComparisonOperator) -> DataFrame: ...
    def _compare_to_other_df(self, other: DataFrame, op: ComparisonOperator) -> DataFrame: ...
    def _compare_to_non_df(self, other: Any, op: ComparisonOperator) -> DataFrame: ...
    def _div(self, other: Any, *, floordiv: bool) -> DataFrame: ...
    def _cast_all_from_to(self, df: DataFrame, from_: frozenset[PolarsDataType], to: PolarsDataType) -> DataFrame: ...
    def __floordiv__(self, other: DataFrame | Series | int | float) -> DataFrame: ...
    def __truediv__(self, other: DataFrame | Series | int | float) -> DataFrame: ...
    def __bool__(self) -> NoReturn: ...
    def __eq__(self, other: Any) -> DataFrame: ...
    def __ne__(self, other: Any) -> DataFrame: ...
    def __gt__(self, other: Any) -> DataFrame: ...
    def __lt__(self, other: Any) -> DataFrame: ...
    def __ge__(self, other: Any) -> DataFrame: ...
    def __le__(self, other: Any) -> DataFrame: ...
    def __getstate__(self) -> list[Series]: ...
    def __setstate__(self, state: list[Series]) -> None: ...
    def __mul__(self, other: DataFrame | Series | int | float) -> Self: ...
    def __rmul__(self, other: DataFrame | Series | int | float) -> Self: ...
    def __add__(self, other: DataFrame | Series | int | float | bool | str) -> DataFrame: ...
    def __radd__(self, other: DataFrame | Series | int | float | bool | str) -> DataFrame: ...
    def __sub__(self, other: DataFrame | Series | int | float) -> Self: ...
    def __mod__(self, other: DataFrame | Series | int | float) -> Self: ...
    def __str__(self) -> str: ...
    def __repr__(self) -> str: ...
    def __contains__(self, key: str) -> bool: ...
    def __iter__(self) -> Iterator[Series]: ...
    def __reversed__(self) -> Iterator[Series]: ...
    def _pos_idx(self, idx: int, dim: int) -> int: ...
    def _take_with_series(self, s: Series) -> DataFrame: ...
    @overload
    def __getitem__(self, item: str) -> Series: ...
    @overload
    def __getitem__(self, item: int | np.ndarray[Any, Any] | MultiColSelector | tuple[int, MultiColSelector] | tuple[MultiRowSelector, MultiColSelector]) -> Self: ...
    @overload
    def __getitem__(self, item: tuple[int, int | str]) -> Any: ...
    @overload
    def __getitem__(self, item: tuple[MultiRowSelector, int | str]) -> Series: ...
    def __setitem__(self, key: str | Sequence[int] | Sequence[str] | tuple[Any, str | int], value: Any) -> None: ...
    def __len__(self) -> int: ...
    def __copy__(self) -> Self: ...
    def __deepcopy__(self, memo: None = ...) -> Self: ...
    def _ipython_key_completions_(self) -> list[str]: ...
    def _repr_html_(self, **kwargs: Any) -> str: ...
    def item(self, row: int | None = ..., column: int | str | None = ...) -> Any: ...
    def to_arrow(self) -> pa.Table: ...
    @overload
    def to_dict(self, as_series: Literal[True] = ...) -> dict[str, Series]: ...
    @overload
    def to_dict(self, as_series: Literal[False]) -> dict[str, list[Any]]: ...
    @overload
    def to_dict(self, as_series: bool) -> dict[str, Series] | dict[str, list[Any]]: ...
    def to_dicts(self) -> list[dict[str, Any]]: ...
    def to_numpy(self, structured: bool = ..., *, order: IndexOrder = ...) -> np.ndarray[Any, Any]: ...
    def to_pandas(self, *args: Any, use_pyarrow_extension_array: bool = ..., **kwargs: Any) -> pd.DataFrame: ...
    def to_series(self, index: int = ...) -> Series: ...
    def to_init_repr(self, n: int = ...) -> str: ...
    @overload
    def write_json(self, file: None = ..., *, pretty: bool = ..., row_oriented: bool = ...) -> str: ...
    @overload
    def write_json(self, file: IOBase | str | Path, *, pretty: bool = ..., row_oriented: bool = ...) -> None: ...
    @overload
    def write_ndjson(self, file: None = ...) -> str: ...
    @overload
    def write_ndjson(self, file: IOBase | str | Path) -> None: ...
    @overload
    def write_csv(self, file: None = ..., *, has_header: bool = ..., separator: str = ..., line_terminator: str = ..., quote_char: str = ..., batch_size: int = ..., datetime_format: str | None = ..., date_format: str | None = ..., time_format: str | None = ..., float_precision: int | None = ..., null_value: str | None = ..., quote_style: CsvQuoteStyle | None = ...) -> str: ...
    @overload
    def write_csv(self, file: BytesIO | TextIOWrapper | str | Path, *, has_header: bool = ..., separator: str = ..., line_terminator: str = ..., quote_char: str = ..., batch_size: int = ..., datetime_format: str | None = ..., date_format: str | None = ..., time_format: str | None = ..., float_precision: int | None = ..., null_value: str | None = ..., quote_style: CsvQuoteStyle | None = ...) -> None: ...
    def write_avro(self, file: BinaryIO | BytesIO | str | Path, compression: AvroCompression = ...) -> None: ...
    def write_excel(self, workbook: Workbook | BytesIO | Path | str | None = ..., worksheet: str | None = ..., *, position: tuple[int, int] | str = ..., table_style: str | dict[str, Any] | None = ..., table_name: str | None = ..., column_formats: ColumnFormatDict | None = ..., dtype_formats: dict[OneOrMoreDataTypes, str] | None = ..., conditional_formats: ConditionalFormatDict | None = ..., header_format: dict[str, Any] | None = ..., column_totals: ColumnTotalsDefinition | None = ..., column_widths: ColumnWidthsDefinition | None = ..., row_totals: RowTotalsDefinition | None = ..., row_heights: dict[int | tuple[int, ...], int] | int | None = ..., sparklines: dict[str, Sequence[str] | dict[str, Any]] | None = ..., formulas: dict[str, str | dict[str, str]] | None = ..., float_precision: int = ..., has_header: bool = ..., autofilter: bool = ..., autofit: bool = ..., hidden_columns: Sequence[str] | SelectorType | None = ..., hide_gridlines: bool = ..., sheet_zoom: int | None = ..., freeze_panes: str | tuple[int, int] | tuple[str, int, int] | tuple[int, int, int, int] | None = ...) -> Workbook: ...
    @overload
    def write_ipc(self, file: None, compression: IpcCompression = ...) -> BytesIO: ...
    @overload
    def write_ipc(self, file: BinaryIO | BytesIO | str | Path, compression: IpcCompression = ...) -> None: ...
    @overload
    def write_ipc_stream(self, file: None, compression: IpcCompression = ...) -> BytesIO: ...
    @overload
    def write_ipc_stream(self, file: BinaryIO | BytesIO | str | Path, compression: IpcCompression = ...) -> None: ...
    def write_parquet(self, file: str | Path | BytesIO, *, compression: ParquetCompression = ..., compression_level: int | None = ..., statistics: bool = ..., row_group_size: int | None = ..., use_pyarrow: bool = ..., pyarrow_options: dict[str, Any] | None = ...) -> None: ...
    def write_database(self, table_name: str, connection: str, *, if_exists: DbWriteMode = ..., engine: DbWriteEngine = ...) -> None: ...
    def write_delta(self, target: str | Path | deltalake.DeltaTable, *, mode: Literal['error', 'append', 'overwrite', 'ignore'] = ..., overwrite_schema: bool = ..., storage_options: dict[str, str] | None = ..., delta_write_options: dict[str, Any] | None = ...) -> None: ...
    def estimated_size(self, unit: SizeUnit = ...) -> int | float: ...
    def transpose(self, *, include_header: bool = ..., header_name: str = ..., column_names: str | Iterable[str] | None = ...) -> Self: ...
    def reverse(self) -> DataFrame: ...
    def rename(self, mapping: dict[str, str]) -> DataFrame: ...
    def insert_at_idx(self, index: int, series: Series) -> Self: ...
    def filter(self, *predicates: IntoExprColumn | Iterable[IntoExprColumn] | bool | list[bool] | np.ndarray[Any, Any], **constraints: Any) -> DataFrame: ...
    @overload
    def glimpse(self, *, max_items_per_column: int = ..., max_colname_length: int = ..., return_as_string: Literal[False]) -> None: ...
    @overload
    def glimpse(self, *, max_items_per_column: int = ..., max_colname_length: int = ..., return_as_string: Literal[True]) -> str: ...
    def describe(self, percentiles: Sequence[float] | float | None = ...) -> Self: ...
    def find_idx_by_name(self, name: str) -> int: ...
    def replace_at_idx(self, index: int, series: Series) -> Self: ...
    def sort(self, by: IntoExpr | Iterable[IntoExpr], *more_by: IntoExpr, descending: bool | Sequence[bool] = ..., nulls_last: bool = ...) -> DataFrame: ...
    def top_k(self, k: int, *, by: IntoExpr | Iterable[IntoExpr], descending: bool | Sequence[bool] = ..., nulls_last: bool = ..., maintain_order: bool = ...) -> DataFrame: ...
    def bottom_k(self, k: int, *, by: IntoExpr | Iterable[IntoExpr], descending: bool | Sequence[bool] = ..., nulls_last: bool = ..., maintain_order: bool = ...) -> DataFrame: ...
    def frame_equal(self, other: DataFrame, *, null_equal: bool = ...) -> bool: ...
    def replace(self, column: str, new_column: Series) -> Self: ...
    def slice(self, offset: int, length: int | None = ...) -> Self: ...
    def head(self, n: int = ...) -> Self: ...
    def tail(self, n: int = ...) -> Self: ...
    def limit(self, n: int = ...) -> Self: ...
    def drop_nulls(self, subset: ColumnNameOrSelector | Collection[ColumnNameOrSelector] | None = ...) -> DataFrame: ...
    def pipe(self, function: Callable[Concatenate[DataFrame, P], T], *args: P.args, **kwargs: P.kwargs) -> T: ...
    def with_row_count(self, name: str = ..., offset: int = ...) -> Self: ...
    def group_by(self, by: IntoExpr | Iterable[IntoExpr], *more_by: IntoExpr, maintain_order: bool = ...) -> GroupBy: ...
    def rolling(self, index_column: IntoExpr, *, period: str | timedelta, offset: str | timedelta | None = ..., closed: ClosedInterval = ..., by: IntoExpr | Iterable[IntoExpr] | None = ..., check_sorted: bool = ...) -> RollingGroupBy: ...
    def group_by_dynamic(self, index_column: IntoExpr, *, every: str | timedelta, period: str | timedelta | None = ..., offset: str | timedelta | None = ..., truncate: bool | None = ..., include_boundaries: bool = ..., closed: ClosedInterval = ..., label: Label = ..., by: IntoExpr | Iterable[IntoExpr] | None = ..., start_by: StartBy = ..., check_sorted: bool = ...) -> DynamicGroupBy: ...
    def upsample(self, time_column: str, *, every: str | timedelta, offset: str | timedelta | None = ..., by: str | Sequence[str] | None = ..., maintain_order: bool = ...) -> Self: ...
    def join_asof(self, other: DataFrame, *, left_on: str | None | Expr = ..., right_on: str | None | Expr = ..., on: str | None | Expr = ..., by_left: str | Sequence[str] | None = ..., by_right: str | Sequence[str] | None = ..., by: str | Sequence[str] | None = ..., strategy: AsofJoinStrategy = ..., suffix: str = ..., tolerance: str | int | float | timedelta | None = ..., allow_parallel: bool = ..., force_parallel: bool = ...) -> DataFrame: ...
    def join(self, other: DataFrame, on: str | Expr | Sequence[str | Expr] | None = ..., how: JoinStrategy = ..., *, left_on: str | Expr | Sequence[str | Expr] | None = ..., right_on: str | Expr | Sequence[str | Expr] | None = ..., suffix: str = ..., validate: JoinValidation = ...) -> DataFrame: ...
    def map_rows(self, function: Callable[[tuple[Any, ...]], Any], return_dtype: PolarsDataType | None = ..., *, inference_size: int = ...) -> DataFrame: ...
    def hstack(self, columns: list[Series] | DataFrame, *, in_place: bool = ...) -> Self: ...
    def vstack(self, other: DataFrame, *, in_place: bool = ...) -> Self: ...
    def extend(self, other: DataFrame) -> Self: ...
    def drop(self, columns: ColumnNameOrSelector | Collection[ColumnNameOrSelector], *more_columns: ColumnNameOrSelector) -> DataFrame: ...
    def drop_in_place(self, name: str) -> Series: ...
    def cast(self, dtypes: Mapping[ColumnNameOrSelector, PolarsDataType] | PolarsDataType, *, strict: bool = ...) -> DataFrame: ...
    def clear(self, n: int = ...) -> Self: ...
    def clone(self) -> Self: ...
    def get_columns(self) -> list[Series]: ...
    def get_column(self, name: str) -> Series: ...
    def fill_null(self, value: Any | None = ..., strategy: FillNullStrategy | None = ..., limit: int | None = ..., *, matches_supertype: bool = ...) -> DataFrame: ...
    def fill_nan(self, value: Expr | int | float | None) -> DataFrame: ...
    def explode(self, columns: str | Expr | Sequence[str | Expr], *more_columns: str | Expr) -> DataFrame: ...
    def pivot(self, values: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None, index: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None, columns: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None, aggregate_function: PivotAgg | Expr | None = ..., *, maintain_order: bool = ..., sort_columns: bool = ..., separator: str = ...) -> Self: ...
    def melt(self, id_vars: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None = ..., value_vars: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None = ..., variable_name: str | None = ..., value_name: str | None = ...) -> Self: ...
    def unstack(self, step: int, how: UnstackDirection = ..., columns: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None = ..., fill_values: list[Any] | None = ...) -> DataFrame: ...
    @overload
    def partition_by(self, by: ColumnNameOrSelector | Sequence[ColumnNameOrSelector], *more_by: str, maintain_order: bool = ..., include_key: bool = ..., as_dict: Literal[False] = ...) -> list[Self]: ...
    @overload
    def partition_by(self, by: ColumnNameOrSelector | Sequence[ColumnNameOrSelector], *more_by: str, maintain_order: bool = ..., include_key: bool = ..., as_dict: Literal[True]) -> dict[Any, Self]: ...
    def shift(self, n: int = ..., *, fill_value: IntoExpr | None = ...) -> DataFrame: ...
    def is_duplicated(self) -> Series: ...
    def is_unique(self) -> Series: ...
    def lazy(self) -> LazyFrame: ...
    def select(self, *exprs: IntoExpr | Iterable[IntoExpr], **named_exprs: IntoExpr) -> DataFrame: ...
    def select_seq(self, *exprs: IntoExpr | Iterable[IntoExpr], **named_exprs: IntoExpr) -> DataFrame: ...
    def with_columns(self, *exprs: IntoExpr | Iterable[IntoExpr], **named_exprs: IntoExpr) -> DataFrame: ...
    def with_columns_seq(self, *exprs: IntoExpr | Iterable[IntoExpr], **named_exprs: IntoExpr) -> DataFrame: ...
    @overload
    def n_chunks(self, strategy: Literal['first'] = ...) -> int: ...
    @overload
    def n_chunks(self, strategy: Literal['all']) -> list[int]: ...
    @overload
    def max(self, axis: Literal[0] = ...) -> Self: ...
    @overload
    def max(self, axis: Literal[1]) -> Series: ...
    @overload
    def max(self, axis: int = ...) -> Self | Series: ...
    @overload
    def min(self, axis: Literal[0] = ...) -> Self: ...
    @overload
    def min(self, axis: Literal[1]) -> Series: ...
    @overload
    def min(self, axis: int = ...) -> Self | Series: ...
    @overload
    def sum(self, *, axis: Literal[0] = ..., null_strategy: NullStrategy = ...) -> Self: ...
    @overload
    def sum(self, *, axis: Literal[1], null_strategy: NullStrategy = ...) -> Series: ...
    @overload
    def sum(self, *, axis: int = ..., null_strategy: NullStrategy = ...) -> Self | Series: ...
    @overload
    def mean(self, *, axis: Literal[0] = ..., null_strategy: NullStrategy = ...) -> Self: ...
    @overload
    def mean(self, *, axis: Literal[1], null_strategy: NullStrategy = ...) -> Series: ...
    @overload
    def mean(self, *, axis: int = ..., null_strategy: NullStrategy = ...) -> Self | Series: ...
    def std(self, ddof: int = ...) -> Self: ...
    def var(self, ddof: int = ...) -> Self: ...
    def median(self) -> Self: ...
    def product(self) -> DataFrame: ...
    def quantile(self, quantile: float, interpolation: RollingInterpolationMethod = ...) -> Self: ...
    def to_dummies(self, columns: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None = ..., *, separator: str = ..., drop_first: bool = ...) -> Self: ...
    def unique(self, subset: ColumnNameOrSelector | Collection[ColumnNameOrSelector] | None = ..., *, keep: UniqueKeepStrategy = ..., maintain_order: bool = ...) -> DataFrame: ...
    def n_unique(self, subset: str | Expr | Sequence[str | Expr] | None = ...) -> int: ...
    def approx_n_unique(self) -> DataFrame: ...
    def approx_unique(self) -> DataFrame: ...
    def rechunk(self) -> Self: ...
    def null_count(self) -> Self: ...
    def sample(self, n: int | Series | None = ..., *, fraction: float | Series | None = ..., with_replacement: bool = ..., shuffle: bool = ..., seed: int | None = ...) -> Self: ...
    def fold(self, operation: Callable[[Series, Series], Series]) -> Series: ...
    @overload
    def row(self, index: int | None = ..., *, by_predicate: Expr | None = ..., named: Literal[False] = ...) -> tuple[Any, ...]: ...
    @overload
    def row(self, index: int | None = ..., *, by_predicate: Expr | None = ..., named: Literal[True]) -> dict[str, Any]: ...
    @overload
    def rows(self, *, named: Literal[False] = ...) -> list[tuple[Any, ...]]: ...
    @overload
    def rows(self, *, named: Literal[True]) -> list[dict[str, Any]]: ...
    def rows_by_key(self, key: ColumnNameOrSelector | Sequence[ColumnNameOrSelector], *, named: bool = ..., include_key: bool = ..., unique: bool = ...) -> dict[Any, Iterable[Any]]: ...
    @overload
    def iter_rows(self, *, named: Literal[False] = ..., buffer_size: int = ...) -> Iterator[tuple[Any, ...]]: ...
    @overload
    def iter_rows(self, *, named: Literal[True], buffer_size: int = ...) -> Iterator[dict[str, Any]]: ...
    def iter_slices(self, n_rows: int = ...) -> Iterator[DataFrame]: ...
    def shrink_to_fit(self, *, in_place: bool = ...) -> Self: ...
    def take_every(self, n: int) -> DataFrame: ...
    def hash_rows(self, seed: int = ..., seed_1: int | None = ..., seed_2: int | None = ..., seed_3: int | None = ...) -> Series: ...
    def interpolate(self) -> DataFrame: ...
    def is_empty(self) -> bool: ...
    def to_struct(self, name: str) -> Series: ...
    def unnest(self, columns: ColumnNameOrSelector | Collection[ColumnNameOrSelector], *more_columns: ColumnNameOrSelector) -> Self: ...
    def corr(self, **kwargs: Any) -> DataFrame: ...
    def merge_sorted(self, other: DataFrame, key: str) -> DataFrame: ...
    def set_sorted(self, column: str | Iterable[str], *more_columns: str, descending: bool = ...) -> DataFrame: ...
    def update(self, other: DataFrame, on: str | Sequence[str] | None = ..., left_on: str | Sequence[str] | None = ..., right_on: str | Sequence[str] | None = ..., how: Literal['left', 'inner', 'outer'] = ..., include_nulls: bool | None = ...) -> DataFrame: ...
    def groupby(self, by: IntoExpr | Iterable[IntoExpr], *more_by: IntoExpr, maintain_order: bool = ...) -> GroupBy: ...
    def groupby_rolling(self, index_column: IntoExpr, *, period: str | timedelta, offset: str | timedelta | None = ..., closed: ClosedInterval = ..., by: IntoExpr | Iterable[IntoExpr] | None = ..., check_sorted: bool = ...) -> RollingGroupBy: ...
    def group_by_rolling(self, index_column: IntoExpr, *, period: str | timedelta, offset: str | timedelta | None = ..., closed: ClosedInterval = ..., by: IntoExpr | Iterable[IntoExpr] | None = ..., check_sorted: bool = ...) -> RollingGroupBy: ...
    def groupby_dynamic(self, index_column: IntoExpr, *, every: str | timedelta, period: str | timedelta | None = ..., offset: str | timedelta | None = ..., truncate: bool = ..., include_boundaries: bool = ..., closed: ClosedInterval = ..., by: IntoExpr | Iterable[IntoExpr] | None = ..., start_by: StartBy = ..., check_sorted: bool = ...) -> DynamicGroupBy: ...
    def apply(self, function: Callable[[tuple[Any, ...]], Any], return_dtype: PolarsDataType | None = ..., *, inference_size: int = ...) -> DataFrame: ...
    def shift_and_fill(self, fill_value: int | str | float, *, n: int = ...) -> DataFrame: ...

def _prepare_other_arg(other: Any, length: int | None = ...) -> Series: ...
